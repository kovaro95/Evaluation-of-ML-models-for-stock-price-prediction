{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "ML_stockm.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "d9w1RXOBFtuq",
        "outputId": "17c7c80d-bd10-4949-a4b1-0c9b3b0bfbbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import pandas as pd \n",
        "import numpy as np\n",
        "from pandas_datareader import data as pdr\n",
        "\n",
        "import yfinance as yf #yahoo finance stock data\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from math import sqrt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "from sklearn.linear_model import LinearRegression,ElasticNet\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Activation,Conv2D,MaxPooling2D,Flatten,Dropout,Dense,LSTM,BatchNormalization\n",
        "\n",
        "stock=\"GOOG\"\n",
        "period=14"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "tags": [],
        "id": "bjMUoGXZFtut"
      },
      "source": [
        "end=datetime.datetime(2019,6,15,0,0,0)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I-gUmUzFOj0G",
        "outputId": "84dee8b7-6a91-40e9-8844-b27d91b97f3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "min=yf.download(tickers=stock,period=\"3d\",enddate=end,interval=\"1m\")\n",
        "thirt_min=yf.download(tickers=stock,period=\"60d\",enddate=end, interval=\"30m\")\n",
        "hour=yf.download(tickers=stock,period=\"150d\",enddate=end,interval=\"1h\")\n",
        "daily=yf.download(tickers=stock,period=\"1000d\",enddate=end,interval=\"1d\")\n",
        "weekly=yf.download(tickers=stock,period=\"3800d\",enddate=end,interval=\"1wk\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n",
            "[*********************100%***********************]  1 of 1 completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTBpXURtFtuw"
      },
      "source": [
        "def Prepare_dataset(df):\n",
        "    from sklearn.model_selection import train_test_split\n",
        "\n",
        "    df.dropna(inplace=True,how=\"all\",axis=0)\n",
        "    df.fillna(inplace=True,method=\"ffill\")\n",
        "    df=df.iloc[-755:]\n",
        "    y=df.shift(-1).iloc[:-1,3].values\n",
        "    X=df.drop([\"Adj Close\",\"Close\"],1)\n",
        "    X=X.iloc[:-1,:]\n",
        "    return train_test_split(X, y, test_size=0.2, random_state=0,shuffle=False)\n",
        "\n",
        "def Rmse(y,y_pred):\n",
        "    return np.sqrt(((y_pred - y) ** 2).mean())\n",
        "\n",
        "def Arima_build(ts_train,ts_test):\n",
        "  def arimamodel(timeseriesarray):\n",
        "    import pmdarima as pmd\n",
        "    autoarima_model = pmd.auto_arima(timeseriesarray, \n",
        "                              start_p=1,\n",
        "                              max_p=5,\n",
        "                              start_q=1,\n",
        "                              max_q=5,\n",
        "                              test=\"adf\",\n",
        "                              max_order=5,\n",
        "                              seasonal_test=\"OCSB\",\n",
        "                              stepwise=False,\n",
        "                              n_jobs=-1,\n",
        "                              trace=True)\n",
        "    return autoarima_model\n",
        "\n",
        "  arima_model = arimamodel(ts_train)\n",
        "  preds=arima_model.predict(len(ts_test))\n",
        "  return ts_test\n",
        "\n",
        "def plot_history(history):\n",
        "    hist=pd.DataFrame(history.history)\n",
        "    hist[\"epoch\"]=history.epoch\n",
        "\n",
        "    plt.figure()\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Mean Abs Error\")\n",
        "    plt.plot(hist.epoch,hist.mae,label=\"Train error\")\n",
        "    plt.plot(hist.epoch,hist.val_mae,label=\"Val error\")\n",
        "    plt.legend()\n",
        "    plt.ylim([0,hist.mae.max()])\n",
        "\n",
        "def Build_lstm(X_train,X_test,y_train,scale=True):\n",
        "    if scale==True:     \n",
        "        x_scaler = StandardScaler()\n",
        "        X_train=x_scaler.fit_transform(X_train)\n",
        "        X_test=x_scaler.transform(X_test)\n",
        "        \n",
        "    x_train=np.array(X_train).reshape(X_train.shape[0],X_train.shape[1],1)\n",
        "    x_test=np.array(X_test).reshape(X_test.shape[0],X_test.shape[1],1)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(LSTM(50, return_sequences=True))\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(LSTM(units=100))\n",
        "    model.add(Dense(units=25))\n",
        "    model.add(Dense(units=1))\n",
        "\n",
        "    model.compile(loss=\"mse\",optimizer=\"nadam\",metrics=['accuracy'])\n",
        "    model.fit(x_train, np.array(y_train), epochs=1000,verbose=0)\n",
        "    preds=model.predict(x_test, batch_size=1)\n",
        "\n",
        "    return preds\n",
        "\n"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtQ3q5lAFtuz"
      },
      "source": [
        "datasets={}\n",
        "datasets[\"1m\"]=Prepare_dataset(min)\n",
        "datasets[\"30m\"]=Prepare_dataset(thirt_min)\n",
        "datasets[\"1h\"]=Prepare_dataset(hour)\n",
        "datasets[\"D\"]=Prepare_dataset(daily)\n",
        "datasets[\"W\"]=Prepare_dataset(weekly)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Bf8VzDYFtu3",
        "outputId": "b1cc2a3f-fb51-4102-a68e-08ba2dd6d25e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "tags": []
      },
      "source": [
        "result=pd.DataFrame(columns=[\"1m\",\"30m\",\"1h\",\"D\",\"W\"])\n",
        "\n",
        "for key in datasets:\n",
        "    X_train=datasets[key][0]\n",
        "    X_test=datasets[key][1]\n",
        "    y_train=datasets[key][2]\n",
        "    y_test=datasets[key][3]\n",
        "\n",
        "    elastic=ElasticNet().fit(X_train, y_train)\n",
        "    elastic_preds=elastic.predict(X_test)\n",
        "    result.loc[\"Elastic net\",key]=[Rmse(y_test,elastic_preds)]\n",
        "\n",
        "    rf=RandomForestRegressor(n_estimators=100,max_depth=12,random_state=0).fit(X_train, y_train)\n",
        "    rf_preds=rf.predict(X_test)\n",
        "    result.loc[\"Random forest\",key]=[Rmse(y_test,rf_preds)]\n",
        "\n",
        "    arima_preds=Arima_build(datasets[key][0][\"Open\"],datasets[key][1][\"Open\"])\n",
        "    result.loc[\"ARIMA\",key]=[Rmse(y_test,arima_preds)]\n",
        "\n",
        "    lstm_preds=Build_lstm(X_train,X_test,y_train,scale=False)\n",
        "    result.loc[\"LSTM\",key]=[Rmse(y_test,lstm_preds)]\n",
        "\n",
        "    lstm_preds=Build_lstm(X_train,X_test,y_train)\n",
        "    \n",
        "    result.loc[\"LSTM scaled\",key]=[Rmse(y_test,lstm_preds)]\n",
        "\n",
        "    print(f\"Model for {key} dataset is done\")\n",
        "result"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total fit time: 4.812 seconds\n",
            "Model for 1m dataset is done\n",
            "Total fit time: 2.110 seconds\n",
            "Model for 30m dataset is done\n",
            "Total fit time: 1.943 seconds\n",
            "Model for 1h dataset is done\n",
            "Total fit time: 2.191 seconds\n",
            "Model for D dataset is done\n",
            "Total fit time: 2.204 seconds\n",
            "Model for W dataset is done\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                 1m                   30m  \\\n",
              "Elastic net     [0.777264013508635]   [6.491790425405427]   \n",
              "Random forest  [2.9410001649633064]  [10.538567466761608]   \n",
              "ARIMA          [0.9013635156414669]   [7.871950982744445]   \n",
              "LSTM           [28.658140351118245]  [210.50967633748073]   \n",
              "LSTM scaled     [2.963835174199114]   [21.40598504150875]   \n",
              "\n",
              "                                 1h                     D  \\\n",
              "Elastic net    [16.202775669932727]   [29.62695854537667]   \n",
              "Random forest  [51.399691553311904]  [106.07085747935972]   \n",
              "ARIMA          [18.178468278333302]   [36.06349620425008]   \n",
              "LSTM           [206.65075261793945]   [155.0501981209455]   \n",
              "LSTM scaled    [119.95835903457228]   [258.2470802691361]   \n",
              "\n",
              "                                  W  \n",
              "Elastic net     [50.38042433856689]  \n",
              "Random forest   [252.7138768062313]  \n",
              "ARIMA          [60.621709585032924]  \n",
              "LSTM           [264.57717180110467]  \n",
              "LSTM scaled    [483.82326719136836]  "
            ],
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>1m</th>\n      <th>30m</th>\n      <th>1h</th>\n      <th>D</th>\n      <th>W</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>Elastic net</th>\n      <td>[0.777264013508635]</td>\n      <td>[6.491790425405427]</td>\n      <td>[16.202775669932727]</td>\n      <td>[29.62695854537667]</td>\n      <td>[50.38042433856689]</td>\n    </tr>\n    <tr>\n      <th>Random forest</th>\n      <td>[2.9410001649633064]</td>\n      <td>[10.538567466761608]</td>\n      <td>[51.399691553311904]</td>\n      <td>[106.07085747935972]</td>\n      <td>[252.7138768062313]</td>\n    </tr>\n    <tr>\n      <th>ARIMA</th>\n      <td>[0.9013635156414669]</td>\n      <td>[7.871950982744445]</td>\n      <td>[18.178468278333302]</td>\n      <td>[36.06349620425008]</td>\n      <td>[60.621709585032924]</td>\n    </tr>\n    <tr>\n      <th>LSTM</th>\n      <td>[28.658140351118245]</td>\n      <td>[210.50967633748073]</td>\n      <td>[206.65075261793945]</td>\n      <td>[155.0501981209455]</td>\n      <td>[264.57717180110467]</td>\n    </tr>\n    <tr>\n      <th>LSTM scaled</th>\n      <td>[2.963835174199114]</td>\n      <td>[21.40598504150875]</td>\n      <td>[119.95835903457228]</td>\n      <td>[258.2470802691361]</td>\n      <td>[483.82326719136836]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "preds=Build_lstm(datasets[\"1m\"][0],datasets[\"1m\"][1],datasets[\"1m\"][2],scale=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1739.128 ],\n",
              "        [1739.1311]], dtype=float32),\n",
              " array([1763.65002441, 1763.29504395]))"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "preds[:2],datasets[\"1m\"][3][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[1739.128 ],\n",
              "        [1739.1311]], dtype=float32),\n",
              " array([1763.65002441, 1763.29504395]))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "preds[:2],datasets[\"1m\"][3][:2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ]
}